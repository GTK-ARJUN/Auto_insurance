{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import time\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <th>ps_ind_09_bin</th>\n",
       "      <th>ps_ind_10_bin</th>\n",
       "      <th>ps_ind_11_bin</th>\n",
       "      <th>ps_ind_12_bin</th>\n",
       "      <th>...</th>\n",
       "      <th>ps_car_11_cat_104</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5.048113</td>\n",
       "      <td>2.281937</td>\n",
       "      <td>-0.006126</td>\n",
       "      <td>-1.146353</td>\n",
       "      <td>1.506130</td>\n",
       "      <td>0.523855</td>\n",
       "      <td>1.736757</td>\n",
       "      <td>1.067024</td>\n",
       "      <td>0.401926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.917107</td>\n",
       "      <td>1.109295</td>\n",
       "      <td>2.435316</td>\n",
       "      <td>1.171819</td>\n",
       "      <td>-0.013408</td>\n",
       "      <td>0.683728</td>\n",
       "      <td>-2.783007</td>\n",
       "      <td>-0.198828</td>\n",
       "      <td>0.478956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.404935</td>\n",
       "      <td>-1.454792</td>\n",
       "      <td>1.516882</td>\n",
       "      <td>1.297032</td>\n",
       "      <td>0.161706</td>\n",
       "      <td>1.158127</td>\n",
       "      <td>0.116747</td>\n",
       "      <td>1.063657</td>\n",
       "      <td>-0.532891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.160598</td>\n",
       "      <td>-0.545594</td>\n",
       "      <td>-1.213880</td>\n",
       "      <td>0.475253</td>\n",
       "      <td>1.026656</td>\n",
       "      <td>-0.227338</td>\n",
       "      <td>-0.033361</td>\n",
       "      <td>0.935005</td>\n",
       "      <td>-0.642617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.849476</td>\n",
       "      <td>-0.703857</td>\n",
       "      <td>-0.210735</td>\n",
       "      <td>-1.249064</td>\n",
       "      <td>-0.840705</td>\n",
       "      <td>-0.110160</td>\n",
       "      <td>0.293483</td>\n",
       "      <td>-1.091242</td>\n",
       "      <td>-0.317573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 219 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  ps_ind_01  ps_ind_03  ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  \\\n",
       "0       0          2          5              0              1              0   \n",
       "1       0          1          7              0              0              1   \n",
       "2       0          5          9              0              0              1   \n",
       "3       0          0          2              1              0              0   \n",
       "4       0          0          0              1              0              0   \n",
       "\n",
       "   ps_ind_09_bin  ps_ind_10_bin  ps_ind_11_bin  ps_ind_12_bin  ...  \\\n",
       "0              0              0              0              0  ...   \n",
       "1              0              0              0              0  ...   \n",
       "2              0              0              0              0  ...   \n",
       "3              0              0              0              0  ...   \n",
       "4              0              0              0              0  ...   \n",
       "\n",
       "   ps_car_11_cat_104         0         1         2         3         4  \\\n",
       "0                  0  5.048113  2.281937 -0.006126 -1.146353  1.506130   \n",
       "1                  0 -2.917107  1.109295  2.435316  1.171819 -0.013408   \n",
       "2                  0  0.404935 -1.454792  1.516882  1.297032  0.161706   \n",
       "3                  1 -1.160598 -0.545594 -1.213880  0.475253  1.026656   \n",
       "4                  0  0.849476 -0.703857 -0.210735 -1.249064 -0.840705   \n",
       "\n",
       "          5         6         7         8  \n",
       "0  0.523855  1.736757  1.067024  0.401926  \n",
       "1  0.683728 -2.783007 -0.198828  0.478956  \n",
       "2  1.158127  0.116747  1.063657 -0.532891  \n",
       "3 -0.227338 -0.033361  0.935005 -0.642617  \n",
       "4 -0.110160  0.293483 -1.091242 -0.317573  \n",
       "\n",
       "[5 rows x 219 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data=data['target']\n",
    "x_data=data.drop(['target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test= train_test_split(x_data, y_data, test_size=0.30, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train = (802925, 218) and the shape of y_train = (802925,)\n",
      "Shape of x_test = (344111, 218) and the shape of y_test = (344111,)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of x_train = {x_train.shape} and the shape of y_train = {y_train.shape}')\n",
    "print(f'Shape of x_test = {x_test.shape} and the shape of y_test = {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating function to calculate precision recall and accuracy\n",
    "def conf_mat(y_act,y_pre):\n",
    "    t0, f1, f0, t1= confusion_matrix(y_act, y_pre).ravel()\n",
    "    accuracy=(t0+t1)/(t0+t1+f0+f1)\n",
    "    precision=(t1)/(t1+f1)\n",
    "    recall=(t1)/(t1+t0)\n",
    "    f1_score=(2*precision*recall)/(precision+recall)\n",
    "    return np.round((accuracy*100, recall*100, precision*100, f1_score*100,t0, f1, f0, t1),2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The Simple Logistic Regression Model seems to have high accuracy. Is that what we need at all? What is the problem with this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the logistic regression = 98.98%\n",
      "\n",
      "Recall of the logistic regression = 49.72%\n",
      "\n",
      "Precision of the logistic regression = 99.62%\n",
      "\n",
      "F1 score of the logistic regression = 66.34%\n",
      "\n",
      "Time to train 24.42 s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "lgr=LogisticRegression()\n",
    "lgr.fit(x_train, y_train)\n",
    "lgr_predicted=lgr.predict(x_test)\n",
    "lgr_acc, lgr_recall, lgr_preci, lgr_f1,t0, f1, f0, t1=conf_mat(y_test, lgr_predicted)\n",
    "tt_time=round(time.time()-start_time,2)\n",
    "mat_lgr=('Logistic_regression', lgr_acc, lgr_recall, lgr_preci, lgr_f1,t0, f1, f0, t1, tt_time)\n",
    "print(f'Accuracy of the logistic regression = {lgr_acc}%\\n')\n",
    "print(f'Recall of the logistic regression = {lgr_recall}%\\n')\n",
    "print(f'Precision of the logistic regression = {lgr_preci}%\\n')\n",
    "print(f'F1 score of the logistic regression = {lgr_f1}%\\n')\n",
    "print(f'Time to train {tt_time} s \\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Though the accuracy is good, the model have low recall value and here our priority is to find the more number of people who wil file a claim, so the recall should be high and it seems that the current value of recall id 49.68\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Why do you think f1-score is 0.0?\n",
    "\n",
    "## unable to comment on this question as the f1 score for logistics regression we found is 66.3 and that's no where near 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. What is the precision and recall score for the model?\n",
    "\n",
    "## Precision score = 99.63 %\n",
    "## Recall score = 49.68 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. What is the most important inference you can draw from the result?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  model is able to recall about 49.5 percent of the data as this is like predicting only the 50 percent of those who can claim the insurance. This method is not preferrable because the positive output have 50 percent chances to be incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. What is the accuracy score and f1-score for the improved Logistic Regression model?\n",
    "\n",
    "## we can try to use logistic regression with penalty =l2 and solver =sag as sag is used for the large data sets.\n",
    "\n",
    "##  SGDclassifier with loss function work as a logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the logistic regression = 99.19%\n",
      "\n",
      "Recall of the logistic regression = 49.69%\n",
      "\n",
      "Precision of the logistic regression = 99.9%\n",
      "\n",
      "F1 score of the logistic regression = 66.37%\n",
      "\n",
      "Time to train 179.8 s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''solver =sag is used for large data sets'''\n",
    "\n",
    "lgr=LogisticRegression(penalty='l2', solver='sag')\n",
    "lgr.fit(x_train, y_train)\n",
    "lgr_predicted=lgr.predict(x_test)\n",
    "lgr_acc, lgr_recall, lgr_preci, lgr_f1,t0, f1, f0, t1=conf_mat(y_test, lgr_predicted)\n",
    "tt_time=round(time.time()-start_time,2)\n",
    "\n",
    "mat_lgr_sag=('Log Reg with sag',lgr_acc, lgr_recall, lgr_preci, lgr_f1,t0, f1, f0, t1, tt_time)\n",
    "print(f'Accuracy of the logistic regression = {lgr_acc}%\\n')\n",
    "print(f'Recall of the logistic regression = {lgr_recall}%\\n')\n",
    "print(f'Precision of the logistic regression = {lgr_preci}%\\n')\n",
    "print(f'F1 score of the logistic regression = {lgr_f1}%\\n')\n",
    "print(f'Time to train {tt_time} s \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the SGDclassifier = 97.59%\n",
      "\n",
      "Recall of the SGDclassifier = 50.17%\n",
      "\n",
      "Precision of the SGDclassifier = 97.36%\n",
      "\n",
      "F1 score of the SGDclassifier = 66.22%\n",
      "\n",
      "Time to train 21.5 s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''SGDclassifier with loss function work as a logistic regression'''\n",
    "start_time=time.time()\n",
    "lgr=SGDClassifier(loss='log', penalty='l2')\n",
    "lgr.fit(x_train,y_train)\n",
    "lgr_predicted=lgr.predict(x_test)\n",
    "lgr_acc, lgr_recall, lgr_preci, lgr_f1,t0, f1, f0, t1=conf_mat(y_test, lgr_predicted)\n",
    "tt_time=round(time.time()-start_time,2)\n",
    "mat_sgd=('SGD classifier', lgr_acc, lgr_recall, lgr_preci, lgr_f1,t0, f1, f0, t1, tt_time)\n",
    "print(f'Accuracy of the SGDclassifier = {lgr_acc}%\\n')\n",
    "print(f'Recall of the SGDclassifier = {lgr_recall}%\\n')\n",
    "print(f'Precision of the SGDclassifier = {lgr_preci}%\\n')\n",
    "print(f'F1 score of the SGDclassifier = {lgr_f1}%\\n')\n",
    "print(f'Time to train {tt_time} s \\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For SGDclassifier(logistic regression as loss=log) we see that accuracy=97.7% and F1 score = 66.19%\n",
    "\n",
    "# 6. Why do you think f1-score has improved?\n",
    "\n",
    "## it doesn't look like there is any significant change in the F1 score, however it can be observed that the accuracy has a negligible decline and the there is a negligible rise on the recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. For model LinearSVC play with parameters â€“ dual, max_iter and see if there is any improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the LinearSVC = 99.17%\n",
      "\n",
      "Recall of the LinearSVC = 49.67%\n",
      "\n",
      "Precision of the LinearSVC = 99.91%\n",
      "\n",
      "F1 score of the LinearSVC = 66.36%\n",
      "\n",
      "Time to train 81.46 s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lin_svc=LinearSVC()\n",
    "start_time=time.time()\n",
    "lin_svc.fit(x_train, y_train)\n",
    "lin_svc_predict=lin_svc.predict(x_test)\n",
    "svc_acc, svc_recall, svc_preci, svc_f1,t0, f1, f0, t1=conf_mat(y_test, lin_svc_predict)\n",
    "tt_time=round(time.time()-start_time,2)\n",
    "mat_svc=('SVC', svc_acc, svc_recall, svc_preci, svc_f1,t0, f1, f0, t1, tt_time)\n",
    "print(f'Accuracy of the LinearSVC = {svc_acc}%\\n')\n",
    "print(f'Recall of the LinearSVC = {svc_recall}%\\n')\n",
    "print(f'Precision of the LinearSVC = {svc_preci}%\\n')\n",
    "print(f'F1 score of the LinearSVC = {svc_f1}%\\n')\n",
    "print(f'Time to train {tt_time} s \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmax_iter=[500, 1000, 1500, 2000, 2500, 3000]\\nfor j in max_iter:\\n    print(f'dual = True and max_iter = {j} \\n')\\n    model=LinearSVC(max_iter=j)\\n    model.fit(x_train,y_train)\\n    lin_svc_predict=lin_svc.predict(x_test)\\n    svc_acc, svc_recall, svc_preci, svc_f1=conf_mat(y_test, lin_svc_predict)\\n    print(f'Accuracy of the LinearSVC = {svc_acc}%\\n')\\n    print(f'Recall of the LinearSVC = {svc_recall}%\\n')\\n    print(f'Precision of the LinearSVC = {svc_preci}%\\n')\\n    print(f'F1 score of the LinearSVC = {svc_f1}%\\n')\\n    print('-X-X'*30)\\n\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "max_iter=[500, 1000, 1500, 2000, 2500, 3000]\n",
    "for j in max_iter:\n",
    "    print(f'dual = True and max_iter = {j} \\n')\n",
    "    model=LinearSVC(max_iter=j)\n",
    "    model.fit(x_train,y_train)\n",
    "    lin_svc_predict=lin_svc.predict(x_test)\n",
    "    svc_acc, svc_recall, svc_preci, svc_f1=conf_mat(y_test, lin_svc_predict)\n",
    "    print(f'Accuracy of the LinearSVC = {svc_acc}%\\n')\n",
    "    print(f'Recall of the LinearSVC = {svc_recall}%\\n')\n",
    "    print(f'Precision of the LinearSVC = {svc_preci}%\\n')\n",
    "    print(f'F1 score of the LinearSVC = {svc_f1}%\\n')\n",
    "    print('-X-X'*30)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No significant change was seen in the performance for different values of iteration and dual = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nValueError: Unsupported set of arguments: The combination of penalty='l2' \\nand loss='hinge' are not supported when dual=<class 'bool'>, \\nParameters: penalty='l2', loss='hinge', dual=<class 'bool'>\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "max_iter=[100, 200, 300, 400,500 ]\n",
    "for j in max_iter:\n",
    "    print(f'dual = bool and max_iter = {j} \\n')\n",
    "    model=LinearSVC(dual=bool, max_iter=j, loss='hinge', penalty='l2')\n",
    "    model.fit(x_train,y_train)\n",
    "    lin_svc_predict=lin_svc.predict(x_test)\n",
    "    svc_acc, svc_recall, svc_preci, svc_f1=conf_mat(y_test, lin_svc_predict)\n",
    "    print(f'Accuracy of the LinearSVC = {svc_acc}%\\n')\n",
    "    print(f'Recall of the LinearSVC = {svc_recall}%\\n')\n",
    "    print(f'Precision of the LinearSVC = {svc_preci}%\\n')\n",
    "    print(f'F1 score of the LinearSVC = {svc_f1}%\\n')\n",
    "    print('-X-X'*30)\"\"\"\n",
    "### using dual =bool gives an error for l1 and l2 combination with hinge and squared hinge as n_samples> n_features\n",
    "'''\n",
    "ValueError: Unsupported set of arguments: The combination of penalty='l2' \n",
    "and loss='hinge' are not supported when dual=<class 'bool'>, \n",
    "Parameters: penalty='l2', loss='hinge', dual=<class 'bool'>\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. SVC with Imbalance Check & Feature Optimization & only 100K Records â†’ is there improvement in scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the SVC = 99.17%\n",
      "\n",
      "Recall of the SVC = 49.67%\n",
      "\n",
      "Precision of the SVC = 99.91%\n",
      "\n",
      "F1 score of the SVC = 66.36%\n",
      "\n",
      "Time to train 249.38 s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "as per the reference it's advised to use LinearSVC above 10k, however SVC is used here to check what happen's for 100k records\n",
    "'''\n",
    "'''taking random 100k records'''\n",
    "'''data set has been balanced and optimized previously'''\n",
    "\n",
    "data_svc=pd.read_csv('data.csv')\n",
    "data_svc=data_svc.sample(n=100000, random_state=1)\n",
    "y_svc=data_svc['target']\n",
    "x_svc=data_svc.drop(['target'], axis=1)\n",
    "xsvc_train, xsvc_test, ysvc_train, ysvc_test= train_test_split(x_svc, y_svc, test_size=0.30, random_state=0)\n",
    "from sklearn.svm import SVC\n",
    "model_svc=SVC()\n",
    "\n",
    "start_time=time.time()\n",
    "model_svc.fit(xsvc_train, ysvc_train)\n",
    "ssvc_predict=model_svc.predict(xsvc_test)\n",
    "\n",
    "ssvc_acc, ssvc_recall, ssvc_preci, ssvc_f1,t0, f1, f0, t1=conf_mat(ysvc_test, ssvc_predict)\n",
    "tt_time=round(time.time()-start_time,2)\n",
    "mat_xscv=('SVC with 100',ssvc_acc, ssvc_recall, ssvc_preci, ssvc_f1,t0, f1, f0, t1, tt_time)\n",
    "print(f'Accuracy of the SVC = {svc_acc}%\\n')\n",
    "print(f'Recall of the SVC = {svc_recall}%\\n')\n",
    "print(f'Precision of the SVC = {svc_preci}%\\n')\n",
    "print(f'F1 score of the SVC = {svc_f1}%\\n')\n",
    "print(f'Time to train {tt_time} s \\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It doesn't look like the model has imporved as the recall and f1 is all the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. XGBoost is one the better classifiers -- but still f1-score is very low. What could be the reason?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the xgb = 97.71%\n",
      "\n",
      "Recall of the xgb = 49.96%\n",
      "\n",
      "Precision of the xgb = 97.89%\n",
      "\n",
      "F1 score of the xgb = 66.16%\n",
      "\n",
      "Time to train 274.8 s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_class=XGBClassifier()\n",
    "start_time=time.time()\n",
    "xgb_class.fit(x_train, y_train)\n",
    "xgb_predict=xgb_class.predict(x_test)\n",
    "xgb_acc, xgb_recall, xgb_preci, xgb_f1,t0, f1, f0, t1=conf_mat(y_test, xgb_predict)\n",
    "tt_time=round(time.time()-start_time,2)\n",
    "mat_xgb=('XGBclassifier',xgb_acc, xgb_recall, xgb_preci, xgb_f1, t0, f1, f0, t1,tt_time)\n",
    "print(f'Accuracy of the xgb = {xgb_acc}%\\n')\n",
    "print(f'Recall of the xgb = {xgb_recall}%\\n')\n",
    "print(f'Precision of the xgb = {xgb_preci}%\\n')\n",
    "print(f'F1 score of the xgb = {xgb_f1}%\\n')\n",
    "print(f'Time to train {tt_time} s \\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. What is the increase in number of features after one-hot encoding of the data?\n",
    "\n",
    "## in Q16 of EDA, we used get_dummies rather than onehotencoding for categorical values.\n",
    "## There are 14 catergorical features in the original dataset\n",
    "## after getting dummies and dropping first column we had 177 features\n",
    "## Total features =177+14=191\n",
    "## increase in feature after onehot encoding would be = 191-14= 177"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Do you think using AdaBoost can give any significant improvement over XGBoost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the abc = 84.75%\n",
      "\n",
      "Recall of the abc = 59.05%\n",
      "\n",
      "Precision of the abc = 76.65%\n",
      "\n",
      "F1 score of the abc = 66.71%\n",
      "\n",
      "Time to train 113.21 s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "abc_class=AdaBoostClassifier(n_estimators=30, learning_rate=.01, random_state=0)\n",
    "start_time=time.time()\n",
    "abc_class.fit(x_train, y_train)\n",
    "abc_predict=abc_class.predict(x_test)\n",
    "tt_time=round(time.time()-start_time,2)\n",
    "abc_acc, abc_recall, abc_preci, abc_f1,t0, f1, f0, t1=conf_mat(y_test, abc_predict)\n",
    "tt_time=round(time.time()-start_time,2)\n",
    "mat_ada=('ada_boost',abc_acc, abc_recall, abc_preci, abc_f1, t0, f1, f0, t1,tt_time)\n",
    "print(f'Accuracy of the abc = {abc_acc}%\\n')\n",
    "print(f'Recall of the abc = {abc_recall}%\\n')\n",
    "print(f'Precision of the abc = {abc_preci}%\\n')\n",
    "print(f'F1 score of the abc = {abc_f1}%\\n')\n",
    "print(f'Time to train {tt_time} s \\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. MLPClassifier is the neural network we are trying. But how to choose the right no. of layers and size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.04518623\n",
      "Iteration 2, loss = 0.03325763\n",
      "Iteration 3, loss = 0.03161966\n",
      "Iteration 4, loss = 0.03084302\n",
      "Iteration 5, loss = 0.03003295\n",
      "Iteration 6, loss = 0.02972688\n",
      "Iteration 7, loss = 0.02910629\n",
      "Iteration 8, loss = 0.02885735\n",
      "Iteration 9, loss = 0.02846786\n",
      "Iteration 10, loss = 0.02824646\n",
      "Iteration 11, loss = 0.02792817\n",
      "Iteration 12, loss = 0.02775162\n",
      "Iteration 13, loss = 0.02742802\n",
      "Iteration 14, loss = 0.02737761\n",
      "Iteration 15, loss = 0.02707445\n",
      "Iteration 16, loss = 0.02705204\n",
      "Iteration 17, loss = 0.02682275\n",
      "Iteration 18, loss = 0.02662072\n",
      "Iteration 19, loss = 0.02638084\n",
      "Iteration 20, loss = 0.02622697\n",
      "Iteration 21, loss = 0.02601328\n",
      "Iteration 22, loss = 0.02598664\n",
      "Iteration 23, loss = 0.02565122\n",
      "Iteration 24, loss = 0.02552521\n",
      "Iteration 25, loss = 0.02534948\n",
      "Iteration 26, loss = 0.02510163\n",
      "Iteration 27, loss = 0.02499726\n",
      "Iteration 28, loss = 0.02470229\n",
      "Iteration 29, loss = 0.02451417\n",
      "Iteration 30, loss = 0.02446048\n",
      "Iteration 31, loss = 0.02420210\n",
      "Iteration 32, loss = 0.02408246\n",
      "Iteration 33, loss = 0.02390847\n",
      "Iteration 34, loss = 0.02356223\n",
      "Iteration 35, loss = 0.02360014\n",
      "Iteration 36, loss = 0.02330725\n",
      "Iteration 37, loss = 0.02320641\n",
      "Iteration 38, loss = 0.02299797\n",
      "Iteration 39, loss = 0.02286450\n",
      "Iteration 40, loss = 0.02266062\n",
      "Iteration 41, loss = 0.02241280\n",
      "Iteration 42, loss = 0.02241957\n",
      "Iteration 43, loss = 0.02227323\n",
      "Iteration 44, loss = 0.02209217\n",
      "Iteration 45, loss = 0.02194329\n",
      "Iteration 46, loss = 0.02173584\n",
      "Iteration 47, loss = 0.02162983\n",
      "Iteration 48, loss = 0.02153211\n",
      "Iteration 49, loss = 0.02139412\n",
      "Iteration 50, loss = 0.02119709\n",
      "Iteration 51, loss = 0.02113402\n",
      "Iteration 52, loss = 0.02104600\n",
      "Iteration 53, loss = 0.02099102\n",
      "Iteration 54, loss = 0.02076995\n",
      "Iteration 55, loss = 0.02069946\n",
      "Iteration 56, loss = 0.02056561\n",
      "Iteration 57, loss = 0.02034876\n",
      "Iteration 58, loss = 0.02043122\n",
      "Iteration 59, loss = 0.02029973\n",
      "Iteration 60, loss = 0.02010707\n",
      "Iteration 61, loss = 0.02007973\n",
      "Iteration 62, loss = 0.02013128\n",
      "Iteration 63, loss = 0.01992503\n",
      "Iteration 64, loss = 0.01986392\n",
      "Iteration 65, loss = 0.01964547\n",
      "Iteration 66, loss = 0.01965009\n",
      "Iteration 67, loss = 0.01969409\n",
      "Iteration 68, loss = 0.01958314\n",
      "Iteration 69, loss = 0.01943790\n",
      "Iteration 70, loss = 0.01939578\n",
      "Iteration 71, loss = 0.01932179\n",
      "Iteration 72, loss = 0.01924039\n",
      "Iteration 73, loss = 0.01924516\n",
      "Iteration 74, loss = 0.01902077\n",
      "Iteration 75, loss = 0.01892673\n",
      "Iteration 76, loss = 0.01892884\n",
      "Iteration 77, loss = 0.01883745\n",
      "Iteration 78, loss = 0.01889936\n",
      "Iteration 79, loss = 0.01866582\n",
      "Iteration 80, loss = 0.01869953\n",
      "Iteration 81, loss = 0.01862827\n",
      "Iteration 82, loss = 0.01856812\n",
      "Iteration 83, loss = 0.01867092\n",
      "Iteration 84, loss = 0.01853250\n",
      "Iteration 85, loss = 0.01830634\n",
      "Iteration 86, loss = 0.01835582\n",
      "Iteration 87, loss = 0.01840231\n",
      "Iteration 88, loss = 0.01830804\n",
      "Iteration 89, loss = 0.01817689\n",
      "Iteration 90, loss = 0.01825717\n",
      "Iteration 91, loss = 0.01814877\n",
      "Iteration 92, loss = 0.01798441\n",
      "Iteration 93, loss = 0.01798367\n",
      "Iteration 94, loss = 0.01803864\n",
      "Iteration 95, loss = 0.01799436\n",
      "Iteration 96, loss = 0.01787720\n",
      "Iteration 97, loss = 0.01805058\n",
      "Iteration 98, loss = 0.01768419\n",
      "Iteration 99, loss = 0.01780448\n",
      "Iteration 100, loss = 0.01765563\n",
      "Iteration 101, loss = 0.01772457\n",
      "Iteration 102, loss = 0.01768412\n",
      "Iteration 103, loss = 0.01753447\n",
      "Iteration 104, loss = 0.01749605\n",
      "Iteration 105, loss = 0.01758412\n",
      "Iteration 106, loss = 0.01750121\n",
      "Iteration 107, loss = 0.01744300\n",
      "Iteration 108, loss = 0.01726917\n",
      "Iteration 109, loss = 0.01746373\n",
      "Iteration 110, loss = 0.01739395\n",
      "Iteration 111, loss = 0.01732304\n",
      "Iteration 112, loss = 0.01728660\n",
      "Iteration 113, loss = 0.01731404\n",
      "Iteration 114, loss = 0.01726482\n",
      "Iteration 115, loss = 0.01733362\n",
      "Iteration 116, loss = 0.01715330\n",
      "Iteration 117, loss = 0.01707046\n",
      "Iteration 118, loss = 0.01697240\n",
      "Iteration 119, loss = 0.01703069\n",
      "Iteration 120, loss = 0.01709338\n",
      "Iteration 121, loss = 0.01674712\n",
      "Iteration 122, loss = 0.01701427\n",
      "Iteration 123, loss = 0.01705368\n",
      "Iteration 124, loss = 0.01706263\n",
      "Iteration 125, loss = 0.01690082\n",
      "Iteration 126, loss = 0.01679762\n",
      "Iteration 127, loss = 0.01669909\n",
      "Iteration 128, loss = 0.01678402\n",
      "Iteration 129, loss = 0.01673361\n",
      "Iteration 130, loss = 0.01665136\n",
      "Iteration 131, loss = 0.01645928\n",
      "Iteration 132, loss = 0.01667098\n",
      "Iteration 133, loss = 0.01660678\n",
      "Iteration 134, loss = 0.01654768\n",
      "Iteration 135, loss = 0.01649633\n",
      "Iteration 136, loss = 0.01649591\n",
      "Iteration 137, loss = 0.01638165\n",
      "Iteration 138, loss = 0.01646402\n",
      "Iteration 139, loss = 0.01639034\n",
      "Iteration 140, loss = 0.01632421\n",
      "Iteration 141, loss = 0.01636161\n",
      "Iteration 142, loss = 0.01647484\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy of the mlp = 99.06%\n",
      "\n",
      "Recall of the mlp = 49.89%\n",
      "\n",
      "Precision of the mlp = 99.38%\n",
      "\n",
      "F1 score of the mlp = 66.43%\n",
      "\n",
      "Time to train 2149.34 s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp_model=MLPClassifier(hidden_layer_sizes=(171,100,50), max_iter=700, alpha=0.0001,\n",
    "                       solver='adam', verbose=2, random_state=0, tol=0.0001)\n",
    "start_time=time.time()\n",
    "mlp_model.fit(x_train, y_train)\n",
    "tt_time=round(time.time()-start_time,2)\n",
    "mlp_predict=mlp_model.predict(x_test)\n",
    "mlp_acc, mlp_recall, mlp_preci, mlp_f1,t0, f1, f0, t1=conf_mat(y_test, mlp_predict)\n",
    "tt_time=round(time.time()-start_time,2)\n",
    "mat_mlp=('MLPclassifier',mlp_acc, mlp_recall, mlp_preci, mlp_f1,t0, f1, f0, t1, tt_time)\n",
    "print(f'Accuracy of the mlp = {mlp_acc}%\\n')\n",
    "print(f'Recall of the mlp = {mlp_recall}%\\n')\n",
    "print(f'Precision of the mlp = {mlp_preci}%\\n')\n",
    "print(f'F1 score of the mlp = {mlp_f1}%\\n')\n",
    "print(f'Time to train {tt_time} s \\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the results of mlp with different solver and layers and it doesn't affest much.\n",
    "#### Pint to be noted is that they were done indiividually and the values were mentioned for individual model result.\n",
    "#### Unable to comment on how many layers will be good as the have tried multiple such combination and the results aren't\n",
    "#### varying by a much larger scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nhidden_layer_sizes=(100,100,100,100,100)\\nfor solver = adam \\nTraining loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping at 109 iteration.\\nAccuracy of the mlp = 99.26%\\nRecall of the mlp = 49.76%\\nPrecision of the mlp = 99.83%\\nF1 score of the mlp = 66.42%\\nTime to train 2126.64 s \\n\\nhidden_layer_sizes=(100,100,100,100,100)\\nfor solver = sgd\\nTraining loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping at 74 iteration.\\nAccuracy of the mlp = 99.17%\\nRecall of the mlp = 49.64%\\nPrecision of the mlp = 99.99%\\nF1 score of the mlp = 66.34%\\nTime to train 1065.41 s \\n\\nhidden_layer_sizes=(128,64,32,16,8)\\nsolver='adam'\\nTraining loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping at 144 iter.\\nAccuracy of the mlp = 99.06%\\nRecall of the mlp = 49.91%\\nPrecision of the mlp = 99.34%\\nF1 score of the mlp = 66.44%\\nTime to train 1783.41 s \\n\\nhidden_layer_sizes=(128,64,32,16,8)\\nsolver='sgd'\\nTraining loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping at 53 iter.\\nAccuracy of the mlp = 99.16%\\nRecall of the mlp = 49.82%\\nPrecision of the mlp = 99.61%\\nF1 score of the mlp = 66.42%\\nTime to train 620.03 s \\n\\nhidden_layer_sizes=(256,128,64,32,16)\\nsolver='sgd'\\nTraining loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping at 89 iter.\\nAccuracy of the mlp = 99.25%\\nRecall of the mlp = 49.79%\\nPrecision of the mlp = 99.76%\\nF1 score of the mlp = 66.43%\\nTime to train 1972.04 s \\n\\n\\nsolver='adam'\\nhidden_layer_sizes=(171,100,50)\\nTraining loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping at iter 142.\\nAccuracy of the mlp = 99.06%\\nRecall of the mlp = 49.89%\\nPrecision of the mlp = 99.38%\\nF1 score of the mlp = 66.43%\\nTime to train 2193.26 s \\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "hidden_layer_sizes=(100,100,100,100,100)\n",
    "for solver = adam \n",
    "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping at 109 iteration.\n",
    "Accuracy of the mlp = 99.26%\n",
    "Recall of the mlp = 49.76%\n",
    "Precision of the mlp = 99.83%\n",
    "F1 score of the mlp = 66.42%\n",
    "Time to train 2126.64 s \n",
    "\n",
    "hidden_layer_sizes=(100,100,100,100,100)\n",
    "for solver = sgd\n",
    "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping at 74 iteration.\n",
    "Accuracy of the mlp = 99.17%\n",
    "Recall of the mlp = 49.64%\n",
    "Precision of the mlp = 99.99%\n",
    "F1 score of the mlp = 66.34%\n",
    "Time to train 1065.41 s \n",
    "\n",
    "hidden_layer_sizes=(128,64,32,16,8)\n",
    "solver='adam'\n",
    "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping at 144 iter.\n",
    "Accuracy of the mlp = 99.06%\n",
    "Recall of the mlp = 49.91%\n",
    "Precision of the mlp = 99.34%\n",
    "F1 score of the mlp = 66.44%\n",
    "Time to train 1783.41 s \n",
    "\n",
    "hidden_layer_sizes=(128,64,32,16,8)\n",
    "solver='sgd'\n",
    "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping at 53 iter.\n",
    "Accuracy of the mlp = 99.16%\n",
    "Recall of the mlp = 49.82%\n",
    "Precision of the mlp = 99.61%\n",
    "F1 score of the mlp = 66.42%\n",
    "Time to train 620.03 s \n",
    "\n",
    "hidden_layer_sizes=(256,128,64,32,16)\n",
    "solver='sgd'\n",
    "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping at 89 iter.\n",
    "Accuracy of the mlp = 99.25%\n",
    "Recall of the mlp = 49.79%\n",
    "Precision of the mlp = 99.76%\n",
    "F1 score of the mlp = 66.43%\n",
    "Time to train 1972.04 s \n",
    "\n",
    "\n",
    "solver='adam'\n",
    "hidden_layer_sizes=(171,100,50)\n",
    "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping at iter 142.\n",
    "Accuracy of the mlp = 99.06%\n",
    "Recall of the mlp = 49.89%\n",
    "Precision of the mlp = 99.38%\n",
    "F1 score of the mlp = 66.43%\n",
    "Time to train 2193.26 s \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. At what layer size we get the best f1-score?\n",
    "\n",
    "## solver='adam' and hidden_layer_sizes=(171,100,50) gave good f1 score and recall value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "      <th>Time taken in sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic_regression</td>\n",
       "      <td>98.98</td>\n",
       "      <td>49.72</td>\n",
       "      <td>99.62</td>\n",
       "      <td>66.34</td>\n",
       "      <td>171245.0</td>\n",
       "      <td>638.0</td>\n",
       "      <td>2865.0</td>\n",
       "      <td>169363.0</td>\n",
       "      <td>24.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Log Reg with sag</td>\n",
       "      <td>99.19</td>\n",
       "      <td>49.69</td>\n",
       "      <td>99.90</td>\n",
       "      <td>66.37</td>\n",
       "      <td>171708.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>2622.0</td>\n",
       "      <td>169606.0</td>\n",
       "      <td>179.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLPclassifier</td>\n",
       "      <td>99.06</td>\n",
       "      <td>49.89</td>\n",
       "      <td>99.38</td>\n",
       "      <td>66.43</td>\n",
       "      <td>170818.0</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>170068.0</td>\n",
       "      <td>2149.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SGD classifier</td>\n",
       "      <td>97.59</td>\n",
       "      <td>50.17</td>\n",
       "      <td>97.36</td>\n",
       "      <td>66.22</td>\n",
       "      <td>167323.0</td>\n",
       "      <td>4560.0</td>\n",
       "      <td>3735.0</td>\n",
       "      <td>168493.0</td>\n",
       "      <td>21.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ada_boost</td>\n",
       "      <td>84.75</td>\n",
       "      <td>59.05</td>\n",
       "      <td>76.65</td>\n",
       "      <td>66.71</td>\n",
       "      <td>119413.0</td>\n",
       "      <td>52470.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172228.0</td>\n",
       "      <td>113.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC</td>\n",
       "      <td>99.17</td>\n",
       "      <td>49.67</td>\n",
       "      <td>99.91</td>\n",
       "      <td>66.36</td>\n",
       "      <td>171734.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>2721.0</td>\n",
       "      <td>169507.0</td>\n",
       "      <td>81.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBclassifier</td>\n",
       "      <td>97.71</td>\n",
       "      <td>49.96</td>\n",
       "      <td>97.89</td>\n",
       "      <td>66.16</td>\n",
       "      <td>168258.0</td>\n",
       "      <td>3625.0</td>\n",
       "      <td>4240.0</td>\n",
       "      <td>167988.0</td>\n",
       "      <td>274.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVC with 100</td>\n",
       "      <td>98.51</td>\n",
       "      <td>49.39</td>\n",
       "      <td>99.24</td>\n",
       "      <td>65.95</td>\n",
       "      <td>14957.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>14595.0</td>\n",
       "      <td>249.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  Recall  Precision  F1-score        tn  \\\n",
       "0  Logistic_regression     98.98   49.72      99.62     66.34  171245.0   \n",
       "1     Log Reg with sag     99.19   49.69      99.90     66.37  171708.0   \n",
       "2        MLPclassifier     99.06   49.89      99.38     66.43  170818.0   \n",
       "3       SGD classifier     97.59   50.17      97.36     66.22  167323.0   \n",
       "4            ada_boost     84.75   59.05      76.65     66.71  119413.0   \n",
       "5                  SVC     99.17   49.67      99.91     66.36  171734.0   \n",
       "6        XGBclassifier     97.71   49.96      97.89     66.16  168258.0   \n",
       "7         SVC with 100     98.51   49.39      99.24     65.95   14957.0   \n",
       "\n",
       "        fp      fn        tp  Time taken in sec  \n",
       "0    638.0  2865.0  169363.0              24.42  \n",
       "1    175.0  2622.0  169606.0             179.80  \n",
       "2   1065.0  2160.0  170068.0            2149.34  \n",
       "3   4560.0  3735.0  168493.0              21.50  \n",
       "4  52470.0     0.0  172228.0             113.21  \n",
       "5    149.0  2721.0  169507.0              81.46  \n",
       "6   3625.0  4240.0  167988.0             274.80  \n",
       "7    112.0   336.0   14595.0             249.38  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_all=pd.DataFrame([mat_lgr, mat_lgr_sag, mat_mlp, mat_sgd, mat_ada, mat_svc, mat_xgb, mat_xscv])\n",
    "mat_all.columns=['Model', 'Accuracy', 'Recall', 'Precision', 'F1-score', 'tn', 'fp', 'fn', 'tp', 'Time taken in sec']\n",
    "mat_all.to_csv('model_comp.csv')\n",
    "mat_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. If not missing a positive sample is the priority which model is best so far?\n",
    "\n",
    "## if not missing positive sample is priority, that means the base line is of all positive, here recall has to be considered.\n",
    "\n",
    "## from above table, it can be seen that ada_boost is having a highest recall\n",
    "## Ada_boost should be used for missing less nuber of positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. If not marking negative sample as positive is top priority, which model is best so far?\n",
    "# here we have to check with least percentage of false positive the models\n",
    "\n",
    "#### first model should be selected for getting low fp. \n",
    "#### Results may vary with each iteration with a low variance( found after running multiple times.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "      <th>Time taken in sec</th>\n",
       "      <th>low_fp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC</td>\n",
       "      <td>99.17</td>\n",
       "      <td>49.67</td>\n",
       "      <td>99.91</td>\n",
       "      <td>66.36</td>\n",
       "      <td>171734.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>2721.0</td>\n",
       "      <td>169507.0</td>\n",
       "      <td>81.46</td>\n",
       "      <td>0.0433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Log Reg with sag</td>\n",
       "      <td>99.19</td>\n",
       "      <td>49.69</td>\n",
       "      <td>99.90</td>\n",
       "      <td>66.37</td>\n",
       "      <td>171708.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>2622.0</td>\n",
       "      <td>169606.0</td>\n",
       "      <td>179.80</td>\n",
       "      <td>0.0509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic_regression</td>\n",
       "      <td>98.98</td>\n",
       "      <td>49.72</td>\n",
       "      <td>99.62</td>\n",
       "      <td>66.34</td>\n",
       "      <td>171245.0</td>\n",
       "      <td>638.0</td>\n",
       "      <td>2865.0</td>\n",
       "      <td>169363.0</td>\n",
       "      <td>24.42</td>\n",
       "      <td>0.1854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLPclassifier</td>\n",
       "      <td>99.06</td>\n",
       "      <td>49.89</td>\n",
       "      <td>99.38</td>\n",
       "      <td>66.43</td>\n",
       "      <td>170818.0</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>170068.0</td>\n",
       "      <td>2149.34</td>\n",
       "      <td>0.3095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVC with 100</td>\n",
       "      <td>98.51</td>\n",
       "      <td>49.39</td>\n",
       "      <td>99.24</td>\n",
       "      <td>65.95</td>\n",
       "      <td>14957.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>14595.0</td>\n",
       "      <td>249.38</td>\n",
       "      <td>0.3733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBclassifier</td>\n",
       "      <td>97.71</td>\n",
       "      <td>49.96</td>\n",
       "      <td>97.89</td>\n",
       "      <td>66.16</td>\n",
       "      <td>168258.0</td>\n",
       "      <td>3625.0</td>\n",
       "      <td>4240.0</td>\n",
       "      <td>167988.0</td>\n",
       "      <td>274.80</td>\n",
       "      <td>1.0534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SGD classifier</td>\n",
       "      <td>97.59</td>\n",
       "      <td>50.17</td>\n",
       "      <td>97.36</td>\n",
       "      <td>66.22</td>\n",
       "      <td>167323.0</td>\n",
       "      <td>4560.0</td>\n",
       "      <td>3735.0</td>\n",
       "      <td>168493.0</td>\n",
       "      <td>21.50</td>\n",
       "      <td>1.3252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ada_boost</td>\n",
       "      <td>84.75</td>\n",
       "      <td>59.05</td>\n",
       "      <td>76.65</td>\n",
       "      <td>66.71</td>\n",
       "      <td>119413.0</td>\n",
       "      <td>52470.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172228.0</td>\n",
       "      <td>113.21</td>\n",
       "      <td>15.2480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  Recall  Precision  F1-score        tn  \\\n",
       "5                  SVC     99.17   49.67      99.91     66.36  171734.0   \n",
       "1     Log Reg with sag     99.19   49.69      99.90     66.37  171708.0   \n",
       "0  Logistic_regression     98.98   49.72      99.62     66.34  171245.0   \n",
       "2        MLPclassifier     99.06   49.89      99.38     66.43  170818.0   \n",
       "7         SVC with 100     98.51   49.39      99.24     65.95   14957.0   \n",
       "6        XGBclassifier     97.71   49.96      97.89     66.16  168258.0   \n",
       "3       SGD classifier     97.59   50.17      97.36     66.22  167323.0   \n",
       "4            ada_boost     84.75   59.05      76.65     66.71  119413.0   \n",
       "\n",
       "        fp      fn        tp  Time taken in sec   low_fp  \n",
       "5    149.0  2721.0  169507.0              81.46   0.0433  \n",
       "1    175.0  2622.0  169606.0             179.80   0.0509  \n",
       "0    638.0  2865.0  169363.0              24.42   0.1854  \n",
       "2   1065.0  2160.0  170068.0            2149.34   0.3095  \n",
       "7    112.0   336.0   14595.0             249.38   0.3733  \n",
       "6   3625.0  4240.0  167988.0             274.80   1.0534  \n",
       "3   4560.0  3735.0  168493.0              21.50   1.3252  \n",
       "4  52470.0     0.0  172228.0             113.21  15.2480  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_fp=round((mat_all.fp/(mat_all.fp+ mat_all.fn+mat_all.tp+mat_all.tn))*100,4)\n",
    "mat_all['low_fp']=low_fp\n",
    "low_fp=mat_all\n",
    "low_fp.sort_values(by='low_fp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "### From all the tested methods, it was found that ADA_boost is the best model for predicting whether someone will claim the insurance or not."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
